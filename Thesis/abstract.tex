The problem being addressed in this research is estimating the driver's required attention level from outward-facing dashcam videos. This research takes a different approach to driver attention; assessing the driver's needs as opposed to the driver's performance. The input data consists of outward-facing dashcam videos in various driving situations. A two-stream recurrent convolutional neural network approach is used where one stream, the spatial stream, analyses individual video frames and computes high-level appearance features. The other temporal stream analyses optical flow between adjacent frames and computes high-level motion features. Both spatial and temporal features are then fed into a Recurrent Neural Network (RNN) that explicitly models the sequence of features in time. The dataset being used is a selection of 1750 dashcam videos collected by Chan \emph{et al.}~\cite{Chan2017}. The dataset was previously annotated with positive and negative instances of vehicle accidents, however, for this work a novel annotation is used. Each video is newly annotated with the required attention level of the driver, \emph{i.e.} the amount of attention required to drive in each situation. These videos were collectively annotated among a group of participants who were asked to view the video from the perspective of the driver and provide a label corresponding to what they perceived to be the driver's required attention level. The average score across all participants was taken as the final attention label. The four categories of attention are as follows: low attention, medium attention, high attention, and very high attention. The complete system runs in real-time; approximately ten frames per second.
